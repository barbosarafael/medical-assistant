import textwrap
from chromadb import PersistentClient
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch


# ==============================================
# CONFIGURA√á√ïES
# ==============================================
DB_PATH = "data/vector_db"
COLLECTION_NAME = "medical_docs"
MODEL_NAME = "microsoft/Phi-3-mini-4k-instruct"  # leve, r√°pido, √≥timo para RAG
TOP_K = 8
DEVICE = "cpu"


# ==============================================
# FUN√á√ïES AUXILIARES
# ==============================================

def load_llm(model_name: str):
    """
    Carrega modelo LLM local via Transformers.
    """
    print(f"üß† Carregando modelo local: {model_name} ...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16 if DEVICE == "cuda" else torch.float32,
        device_map=DEVICE,
    )
    print("‚úÖ Modelo carregado!")
    return tokenizer, model


def retrieve_context(query: str, top_k: int = TOP_K):
    """
    Busca chunks relevantes no ChromaDB.
    """
    client = PersistentClient(path=DB_PATH)
    collection = client.get_collection(COLLECTION_NAME)

    results = collection.query(
        query_texts=[query],
        n_results=top_k,
        include=["documents", "metadatas", "distances"]
    )

    context_blocks = []
    for doc, meta in zip(results["documents"][0], results["metadatas"][0]):
        block = f"[Fonte: {meta['font']} | Categoria: {meta['category']}]\n{doc}"
        context_blocks.append(block)

    context = "\n\n---\n\n".join(context_blocks)
    return context, results


def build_prompt(question: str, context: str) -> str:
    """
    Gera o prompt final para o LLM com inje√ß√£o de contexto.
    """
    prompt = f"""
Voc√™ √© um assistente que responde SOMENTE com base nas informa√ß√µes abaixo.
N√£o invente. Se n√£o souber, diga 'N√£o encontrei essa informa√ß√£o nas fontes oficiais.'

### CONTEXTO
{context}

### PERGUNTA DO USU√ÅRIO
{question}

### RESPOSTA (baseada apenas no contexto acima):
"""
    return textwrap.dedent(prompt).strip()


def generate_answer(tokenizer, model, prompt: str) -> str:
    """
    Gera a resposta usando o LLM local.
    """
    inputs = tokenizer(prompt, return_tensors="pt").to(DEVICE)

    output = model.generate(
        **inputs,
        max_new_tokens=350,
        temperature=0.1,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id,
    )

    return tokenizer.decode(output[0], skip_special_tokens=True)


# ==============================================
# MODO PRINCIPAL
# ==============================================
def ask(question: str):
    tokenizer, model = load_llm(MODEL_NAME)

    print(f"\nüîç Pergunta: {question}\n")

    context, results = retrieve_context(question, TOP_K)

    print("üìö Contexto recuperado:")
    print("-" * 80)
    print(context)
    print("-" * 80)

    prompt = build_prompt(question, context)
    answer = generate_answer(tokenizer, model, prompt)

    print("\nü§ñ Resposta:")
    print(answer)


if __name__ == "__main__":
    # Exemplo de pergunta
    ask("Qual o c√≥digo usado para a VACINA ADSORVIDA MENINGOC√ìCICA C?")
